{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL FLORIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name       0\n",
      "time       0\n",
      "rating     0\n",
      "text       0\n",
      "gmap_id    0\n",
      "date       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "directory_florida = '../datasets/dataset_florida'\n",
    "\n",
    "all_data_florida = []\n",
    "\n",
    "for filename in os.listdir(directory_florida):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(directory_florida, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            json_text = file.read()\n",
    "            json_objects = json_text.strip().split('\\n')\n",
    "            \n",
    "            for obj in json_objects:\n",
    "                try:\n",
    "                    all_data_florida.append(json.loads(obj))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f'Error al decodificar JSON en {filename}: {e}')\n",
    "\n",
    "df_florida = pd.DataFrame(all_data_florida)\n",
    "\n",
    "# Convertir la columna 'time' a datetime y extraer la fecha y hora\n",
    "df_florida['time'] = pd.to_datetime(df_florida['time'], unit='ms')\n",
    "df_florida['date'] = df_florida['time'].dt.date\n",
    "df_florida['time'] = df_florida['time'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "# Eliminar columnas no deseadas\n",
    "df_florida.drop(columns=['pics', 'user_id', 'resp'], inplace=True)\n",
    "\n",
    "# Eliminar duplicados\n",
    "df_florida.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# Limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    if texto is None:\n",
    "        return None\n",
    "    if \"(Original)\" in texto:\n",
    "        # Dividir el texto por \"(Original)\" \n",
    "        return texto.split(\"(Original)\")[0].replace(\"(Translated by Google)\", \"\").strip()\n",
    "    else:\n",
    "        return texto\n",
    "\n",
    "df_florida['text'] = df_florida['text'].apply(limpiar_texto)\n",
    "\n",
    "# Rellenar texto basado en la calificación\n",
    "def rellenar_texto(row):\n",
    "    if pd.isnull(row['text']):\n",
    "        ratings = {\n",
    "            1: \"Not recommended\",\n",
    "            2: \"Can be good in certain cases\",\n",
    "            3: \"Good\",\n",
    "            4: \"Very Good\",\n",
    "            5: \"Excellent\"\n",
    "        }\n",
    "        return ratings.get(row['rating'], \"No rating\")  # Retorna \"No rating\" si el rating no está entre 1 y 5\n",
    "    else:\n",
    "        return row['text']\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame\n",
    "df_florida['text'] = df_florida.apply(rellenar_texto, axis=1)\n",
    "\n",
    "# Verificar valores nulos\n",
    "nulos = df_florida.isnull().sum()\n",
    "print(nulos)\n",
    "\n",
    "# Guardar el DataFrame en formato parquet\n",
    "df_florida.to_parquet(r'../datasets/reviews_parquet/florida_reviews.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL ILLINOIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name       0\n",
      "time       0\n",
      "rating     0\n",
      "text       0\n",
      "gmap_id    0\n",
      "date       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "directory_illinois = '../datasets/dataset_illinois'\n",
    "\n",
    "all_Data_illinois = []\n",
    "\n",
    "for filename in os.listdir(directory_illinois):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(directory_illinois, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            json_text = file.read()\n",
    "            json_objects = json_text.strip().split('\\n')\n",
    "            \n",
    "            for obj in json_objects:\n",
    "                try:\n",
    "                    all_Data_illinois.append(json.loads(obj))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f'Error al decodificar JSON en {filename}: {e}')\n",
    "\n",
    "df_illinois = pd.DataFrame(all_Data_illinois)\n",
    "\n",
    "# Convertir la columna 'time' a datetime y extraer la fecha y hora\n",
    "df_illinois['time'] = pd.to_datetime(df_illinois['time'], unit='ms')\n",
    "df_illinois['date'] = df_illinois['time'].dt.date\n",
    "df_illinois['time'] = df_illinois['time'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "# Eliminar columnas no deseadas\n",
    "df_illinois.drop(columns=['pics', 'user_id', 'resp'], inplace=True)\n",
    "\n",
    "# Eliminar duplicados\n",
    "df_illinois.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# Limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    if texto is None:\n",
    "        return None\n",
    "    if \"(Original)\" in texto:\n",
    "        # Dividir el texto por \"(Original)\" \n",
    "        return texto.split(\"(Original)\")[0].replace(\"(Translated by Google)\", \"\").strip()\n",
    "    else:\n",
    "        return texto\n",
    "\n",
    "df_illinois['text'] = df_illinois['text'].apply(limpiar_texto)\n",
    "\n",
    "# Rellenar texto basado en la calificación\n",
    "def rellenar_texto(row):\n",
    "    if pd.isnull(row['text']):\n",
    "        ratings = {\n",
    "            1: \"Not recommended\",\n",
    "            2: \"Can be good in certain cases\",\n",
    "            3: \"Good\",\n",
    "            4: \"Very Good\",\n",
    "            5: \"Excellent\"\n",
    "        }\n",
    "        return ratings.get(row['rating'], \"No rating\")  # Retorna \"No rating\" si el rating no está entre 1 y 5\n",
    "    else:\n",
    "        return row['text']\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame\n",
    "df_illinois['text'] = df_illinois.apply(rellenar_texto, axis=1)\n",
    "\n",
    "# Verificar valores nulos\n",
    "nulos = df_illinois.isnull().sum()\n",
    "print(nulos)\n",
    "\n",
    "# Guardar el DataFrame en formato parquet\n",
    "df_illinois.to_parquet(r'../datasets/reviews_parquet/illinois_reviews.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL NUEVA YORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name       0\n",
      "time       0\n",
      "rating     0\n",
      "text       0\n",
      "gmap_id    0\n",
      "date       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "directory_nuevayork = '../datasets/dataset_nuevayork'\n",
    "\n",
    "all_data_nuevayork = []\n",
    "\n",
    "for filename in os.listdir(directory_nuevayork):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(directory_nuevayork, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            json_text = file.read()\n",
    "            json_objects = json_text.strip().split('\\n')\n",
    "            \n",
    "            for obj in json_objects:\n",
    "                try:\n",
    "                    all_data_nuevayork.append(json.loads(obj))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f'Error al decodificar JSON en {filename}: {e}')\n",
    "\n",
    "df_nuevayork = pd.DataFrame(all_data_nuevayork)\n",
    "\n",
    "# Convertir la columna 'time' a datetime y extraer la fecha y hora\n",
    "df_nuevayork['time'] = pd.to_datetime(df_nuevayork['time'], unit='ms')\n",
    "df_nuevayork['date'] = df_nuevayork['time'].dt.date\n",
    "df_nuevayork['time'] = df_nuevayork['time'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "# Eliminar columnas no deseadas\n",
    "df_nuevayork.drop(columns=['pics', 'user_id', 'resp'], inplace=True)\n",
    "\n",
    "# Eliminar duplicados\n",
    "df_nuevayork.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# Limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    if texto is None:\n",
    "        return None\n",
    "    if \"(Original)\" in texto:\n",
    "        # Dividir el texto por \"(Original)\" \n",
    "        return texto.split(\"(Original)\")[0].replace(\"(Translated by Google)\", \"\").strip()\n",
    "    else:\n",
    "        return texto\n",
    "\n",
    "df_nuevayork['text'] = df_nuevayork['text'].apply(limpiar_texto)\n",
    "\n",
    "# Rellenar texto basado en la calificación\n",
    "def rellenar_texto(row):\n",
    "    if pd.isnull(row['text']):\n",
    "        ratings = {\n",
    "            1: \"Not recommended\",\n",
    "            2: \"Can be good in certain cases\",\n",
    "            3: \"Good\",\n",
    "            4: \"Very Good\",\n",
    "            5: \"Excellent\"\n",
    "        }\n",
    "        return ratings.get(row['rating'], \"No rating\")  # Retorna \"No rating\" si el rating no está entre 1 y 5\n",
    "    else:\n",
    "        return row['text']\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame\n",
    "df_nuevayork['text'] = df_nuevayork.apply(rellenar_texto, axis=1)\n",
    "\n",
    "# Verificar valores nulos\n",
    "nulos = df_nuevayork.isnull().sum()\n",
    "print(nulos)\n",
    "\n",
    "# Guardar el DataFrame en formato parquet\n",
    "df_nuevayork.to_parquet(r'../datasets/reviews_parquet/nuevayork_reviews.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL TEXAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name       0\n",
      "time       0\n",
      "rating     0\n",
      "text       0\n",
      "gmap_id    0\n",
      "date       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "directory_texas = '../datasets/dataset_texas'\n",
    "\n",
    "all_data_texas = []\n",
    "\n",
    "for filename in os.listdir(directory_texas):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(directory_texas, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            json_text = file.read()\n",
    "            json_objects = json_text.strip().split('\\n')\n",
    "            \n",
    "            for obj in json_objects:\n",
    "                try:\n",
    "                    all_data_texas.append(json.loads(obj))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f'Error al decodificar JSON en {filename}: {e}')\n",
    "\n",
    "df_texas = pd.DataFrame(all_data_texas)\n",
    "\n",
    "# Convertir la columna 'time' a datetime y extraer la fecha y hora\n",
    "df_texas['time'] = pd.to_datetime(df_texas['time'], unit='ms')\n",
    "df_texas['date'] = df_texas['time'].dt.date\n",
    "df_texas['time'] = df_texas['time'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "# Eliminar columnas no deseadas\n",
    "df_texas.drop(columns=['pics', 'user_id', 'resp'], inplace=True)\n",
    "\n",
    "# Eliminar duplicados\n",
    "df_texas.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# Limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    if texto is None:\n",
    "        return None\n",
    "    if \"(Original)\" in texto:\n",
    "        # Dividir el texto por \"(Original)\" \n",
    "        return texto.split(\"(Original)\")[0].replace(\"(Translated by Google)\", \"\").strip()\n",
    "    else:\n",
    "        return texto\n",
    "\n",
    "df_texas['text'] = df_texas['text'].apply(limpiar_texto)\n",
    "\n",
    "# Rellenar texto basado en la calificación\n",
    "def rellenar_texto(row):\n",
    "    if pd.isnull(row['text']):\n",
    "        ratings = {\n",
    "            1: \"Not recommended\",\n",
    "            2: \"Can be good in certain cases\",\n",
    "            3: \"Good\",\n",
    "            4: \"Very Good\",\n",
    "            5: \"Excellent\"\n",
    "        }\n",
    "        return ratings.get(row['rating'], \"No rating\")  # Retorna \"No rating\" si el rating no está entre 1 y 5\n",
    "    else:\n",
    "        return row['text']\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame\n",
    "df_texas['text'] = df_texas.apply(rellenar_texto, axis=1)\n",
    "\n",
    "# Verificar valores nulos\n",
    "nulos = df_texas.isnull().sum()\n",
    "print(nulos)\n",
    "\n",
    "# Guardar el DataFrame en formato parquet\n",
    "df_texas.to_parquet(r'../datasets/reviews_parquet/texas_reviews.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL WASHINGTON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name       0\n",
      "time       0\n",
      "rating     0\n",
      "text       0\n",
      "gmap_id    0\n",
      "date       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "directory_washington = '../datasets/dataset_washington'\n",
    "\n",
    "all_data_washington = []\n",
    "\n",
    "for filename in os.listdir(directory_washington):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(directory_washington, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            json_text = file.read()\n",
    "            json_objects = json_text.strip().split('\\n')\n",
    "            \n",
    "            for obj in json_objects:\n",
    "                try:\n",
    "                    all_data_washington.append(json.loads(obj))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f'Error al decodificar JSON en {filename}: {e}')\n",
    "\n",
    "df_washington = pd.DataFrame(all_data_washington)\n",
    "\n",
    "# Convertir la columna 'time' a datetime y extraer la fecha y hora\n",
    "df_washington['time'] = pd.to_datetime(df_washington['time'], unit='ms')\n",
    "df_washington['date'] = df_washington['time'].dt.date\n",
    "df_washington['time'] = df_washington['time'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "# Eliminar columnas no deseadas\n",
    "df_washington.drop(columns=['pics', 'user_id', 'resp'], inplace=True)\n",
    "\n",
    "# Eliminar duplicados\n",
    "df_washington.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# Limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    if texto is None:\n",
    "        return None\n",
    "    if \"(Original)\" in texto:\n",
    "        # Dividir el texto por \"(Original)\" \n",
    "        return texto.split(\"(Original)\")[0].replace(\"(Translated by Google)\", \"\").strip()\n",
    "    else:\n",
    "        return texto\n",
    "\n",
    "df_washington['text'] = df_washington['text'].apply(limpiar_texto)\n",
    "\n",
    "# Rellenar texto basado en la calificación\n",
    "def rellenar_texto(row):\n",
    "    if pd.isnull(row['text']):\n",
    "        ratings = {\n",
    "            1: \"Not recommended\",\n",
    "            2: \"Can be good in certain cases\",\n",
    "            3: \"Good\",\n",
    "            4: \"Very Good\",\n",
    "            5: \"Excellent\"\n",
    "        }\n",
    "        return ratings.get(row['rating'], \"No rating\")  # Retorna \"No rating\" si el rating no está entre 1 y 5\n",
    "    else:\n",
    "        return row['text']\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame\n",
    "df_washington['text'] = df_washington.apply(rellenar_texto, axis=1)\n",
    "\n",
    "# Verificar valores nulos\n",
    "nulos = df_washington.isnull().sum()\n",
    "print(nulos)\n",
    "\n",
    "# Guardar el DataFrame en formato parquet\n",
    "df_washington.to_parquet(r'../datasets/reviews_parquet/washington_reviews.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL CALIFORNIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name       0\n",
      "time       0\n",
      "rating     0\n",
      "text       0\n",
      "gmap_id    0\n",
      "date       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "directory_california = '../datasets/dataset_california'\n",
    "\n",
    "all_data_california = []\n",
    "\n",
    "for filename in os.listdir(directory_california):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(directory_california, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            json_text = file.read()\n",
    "            json_objects = json_text.strip().split('\\n')\n",
    "            \n",
    "            for obj in json_objects:\n",
    "                try:\n",
    "                    all_data_california.append(json.loads(obj))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f'Error al decodificar JSON en {filename}: {e}')\n",
    "\n",
    "df_california = pd.DataFrame(all_data_california)\n",
    "\n",
    "# Convertir la columna 'time' a datetime y extraer la fecha y hora\n",
    "df_california['time'] = pd.to_datetime(df_california['time'], unit='ms')\n",
    "df_california['date'] = df_california['time'].dt.date\n",
    "df_california['time'] = df_california['time'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "# Eliminar columnas no deseadas\n",
    "df_california.drop(columns=['pics', 'user_id', 'resp'], inplace=True)\n",
    "\n",
    "# Eliminar duplicados\n",
    "df_california.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# Limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    if texto is None:\n",
    "        return None\n",
    "    if \"(Original)\" in texto:\n",
    "        # Dividir el texto por \"(Original)\" \n",
    "        return texto.split(\"(Original)\")[0].replace(\"(Translated by Google)\", \"\").strip()\n",
    "    else:\n",
    "        return texto\n",
    "\n",
    "df_california['text'] = df_california['text'].apply(limpiar_texto)\n",
    "\n",
    "# Rellenar texto basado en la calificación\n",
    "def rellenar_texto(row):\n",
    "    if pd.isnull(row['text']):\n",
    "        ratings = {\n",
    "            1: \"Not recommended\",\n",
    "            2: \"Can be good in certain cases\",\n",
    "            3: \"Good\",\n",
    "            4: \"Very Good\",\n",
    "            5: \"Excellent\"\n",
    "        }\n",
    "        return ratings.get(row['rating'], \"No rating\")  # Retorna \"No rating\" si el rating no está entre 1 y 5\n",
    "    else:\n",
    "        return row['text']\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame\n",
    "df_california['text'] = df_california.apply(rellenar_texto, axis=1)\n",
    "\n",
    "# Verificar valores nulos\n",
    "nulos = df_california.isnull().sum()\n",
    "print(nulos)\n",
    "\n",
    "# Guardar el DataFrame en formato parquet\n",
    "df_california.to_parquet(r'../datasets/reviews_parquet/california_reviews.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
